<think>
Okay, I need to improve the RLE compression and decompression functions provided. Let's see what the current code does. The compression function uses a simple RLE where each run is stored as a count followed by the value. The worst case is when there are no runs, so the compressed size could be double the original, which isn't great. The decompression function reads these pairs and expands them.

First, I want to improve the compression ratio. The problem with basic RLE is that it can actually make the data larger if there are no runs. For example, a single byte not followed by the same byte would be stored as two bytes (1 followed by the byte). So, maybe we can modify the algorithm to only encode runs when they're beneficial.

What if we add a flag to indicate whether the next bytes are a run or literal bytes? For example, using a control byte where the highest bit indicates a run. If the highest bit is set, then the next 7 bits represent the run length, followed by the value. If not, the next 7 bits indicate the number of literal bytes following. This way, we can handle both runs and literals, which should help in cases where runs are infrequent.

So for the compression function: when a run is found (same byte repeated), we encode it as (count | 0x80) followed by the byte. But if there are non-repeating bytes, we need to store them as literals. The challenge is to detect when to switch between runs and literals.

But how do we handle the literals? For example, if we have a sequence of non-repeating bytes, we can group them into a literal run. The control byte would have the highest bit 0, and the lower 7 bits would be the count of literals (so up to 127 bytes). Then, those many literals follow.

So during compression, we need to check for runs of at least 3 bytes (since a run of 2 bytes would take 2 control bytes plus the value, which is the same as storing them as literals. Wait, let's see: a run of 2 bytes would be encoded as (2 | 0x80) and the value, which is two bytes. Storing them as literals would be a control byte 2 (since 0x00 in the high bit) followed by two bytes. So same size. So maybe it's better to encode runs of 3 or more. Or maybe even 1 or more, but that might not help. Hmm.

Alternatively, the threshold could be that a run is only encoded if the length is >= 3, because then the control byte (1 byte) + value (1 byte) would take 2 bytes, which is better than storing 3 literals (control byte 3, then 3 bytes, total 4 bytes). So for runs of 3, 4, etc., it's better. So the algorithm would look for runs of 3 or more, and encode as runs. Otherwise, store literals.

So during compression, the loop would need to track the current position and check if the next bytes form a run. If a run of 3 or more is found, encode as a run. Otherwise, collect literals until a run is found or until the maximum literal length (127) is reached.

This approach should help the compression ratio, especially for data with mixed runs and non-repeating sequences.

Let me outline the steps for the compression function:

1. Iterate through the input data.
2. For each position, check the maximum possible run length (same byte repeated up to 255 times, but using 7 bits for the run length, so up to 127? Wait, because the control byte for runs uses 7 bits. Wait, the control byte for runs would have the highest bit set (0x80), and the lower 7 bits for the count. So the maximum run length would be 127. But the original code allowed up to 255. Hmm, but that's a problem. Wait, the original code used a count of up to 255. But with this new approach, the run length is limited to 127 (since 7 bits). So maybe the maximum run length is 127. But that could be a problem if there's a longer run. So for runs longer than 127, we need to split them into multiple run commands.

Alternatively, perhaps adjust the code to allow up to 254 runs (using 7 bits plus 1). Wait, maybe we can use a different approach. For example, if the control byte's lower 7 bits are the count minus 1. So for a run of 1, we can't encode it as a run (since 1-1=0). Wait, maybe the run encoding is (count -1) | 0x80. Then, during decompression, the count is (control_byte & 0x7F) + 1. That way, the maximum run length is 127 +1 = 128. Wait, but 0x7F is 127, so 127 +1 = 128. Hmm, but 128 is 2^7, which is manageable. But even that may not be enough. For example, a run of 200 bytes would require splitting into two runs: 128 and 72. But perhaps it's better to split runs into chunks of up to 128.

But maybe this is more efficient than the original code which allowed runs up to 255. However, the original code's maximum run is 255, but with the new approach, runs are limited to 128. But the original code's run count is stored as a single byte (count), so for runs longer than 255, they would split into multiple counts. Wait, in the original code, the count is a single byte (0-255). So for a run longer than 255, it's split into multiple run entries. For example, a run of 300 would be two entries: 255 and 45. So the same approach applies here.

But in the new approach, the maximum run length per control byte is 128. So for a run of 300, it would split into two runs: 128 and 128, then 44. But each run uses two bytes (control and value), so same as original code's approach for runs over 255. But the original code can handle up to 255 in one entry.

So perhaps in the new approach, the maximum run is 128, which is worse than the original. So maybe this approach isn't better for runs. Hmm. That's a problem. Alternatively, maybe the control byte for runs can have a count that is up to 254. Wait, how?

Alternatively, use the control byte as follows: if the highest bit is set, then the next byte is the count (allowing up to 255), followed by the value. But that would require 3 bytes per run (control byte with a flag, count byte, value byte). But that's more than the original code's 2 bytes per run. That's worse for small runs.

Wait, maybe the original approach is better for runs. But the new approach's goal is to handle both runs and literals. So perhaps the trade-off is that for runs, we use a control byte with the high bit set, followed by the count (using the next 7 bits) plus one, then the value. So the maximum run is 128. For runs longer than 128, split into multiple. But for runs of, say, 3 bytes, that's encoded as 3 (count) | 0x80, then the value. That's two bytes. The original code would also encode that as two bytes (3 and value). So same. But if there's a run of 130, the new code would split into two runs: 128 (count 128, 0x80 | 127 (since 127 +1=128), then the value) and 2 (count 2, 0x80 |1, then the value). So two runs, 4 bytes. The original code would handle 130 as two runs: 255 (if possible) and then 130-255= which is not possible. Wait, original code counts up to 255. So a run of 300 would be 255, then 45, each as two bytes. So total 4 bytes. The new code would split 300 into 128, 128, 44, each as two bytes, total 6 bytes. That's worse. Hmm. So maybe the new approach is not better for long runs.

But perhaps the benefit comes when handling literals. For example, in data with many non-repeating bytes, the original code would expand each single byte into two bytes (1 followed by the byte). The new approach could group literals into chunks. For example, if there are 5 non-repeating bytes, the original code would store 5 pairs of (1, byte), which is 10 bytes. The new approach would store a control byte 5 (0x05) followed by the 5 bytes, total 6 bytes. That's a significant saving.

So the trade-off is between handling runs and literals. The new approach should perform better in cases where there are sequences of non-repeating bytes, at the cost of longer encoding for very long runs. But overall, for many data types, especially those with mixed runs and literals, this should improve the compression ratio.

So now, how to implement this in the code.

For the compression function:

- We need to process the input data, alternating between checking for runs and collecting literals.

The algorithm could be:

Initialize i to 0.

While i < data_size:

   Check the current position for a run. A run is a sequence of at least 3 identical bytes (since a run of 2 would take two bytes as a run (control + value) or two bytes as literals (control + 2 literals). So for a run of 3, it's better to encode as run (2 bytes) vs literals (1 control + 3 bytes =4 bytes). So for runs of 3 or more, it's better.

   So check the maximum possible run starting at i. Let's say the maximum possible run length is up to 128 (since the control byte can store 0-127, which when added to 1, becomes 1-128).

   If a run of length L >=3 is found:

      Encode a run control byte: (L-1) | 0x80. So for L=3, it's 2 |0x80 = 0x82.

      Then write the value.

      Increment i by L.

   Else:

      Collect as many literals as possible, up to 127 bytes (since the control byte for literals uses 7 bits). So find the next sequence of bytes that either ends at the end of the data or has a run starting within the next 127 bytes.

      So from i, collect literals up to j where j is i + 127, or until a run of 3 is found (but need to check for that).

      Wait, but how to decide where to split literals. For example, between i and j, if there's a run starting at position k (within i to j), then we can take literals from i to k-1, then process the run. But that requires look-ahead.

      But that's complicated. Maybe a simpler approach is to collect literals until we find a run of at least 3, or until we reach 127 literals, whichever comes first.

      So start at i. Collect a literal run. The maximum length is 127. So the code can collect from i to i + max_literal_length -1, but during this, check if any of the next bytes form a run of 3. If yes, then split the literal run before the run starts.

      But this requires checking ahead for runs, which can be time-consuming.

      Alternatively, just collect up to 127 bytes, and during that collection, if a run of 3 is encountered, split the literals before the run.

      Hmm. This seems complex. Maybe a greedy approach: collect as many literals as possible up to 127, but if a run of 3 starts within the next 3 bytes, prefer to split.

      For example, start at i. Let's look ahead for the next possible run. If within the next 127 bytes, there's a run of 3 starting at position k (k >=i), then we can take literals from i to k-1, then process the run. Otherwise, take up to 127 literals.

      But implementing this in code could be tricky.

      Alternatively, for simplicity, collect literals until we find a run of 3 starting at the current position. If not found, collect 1 literal and proceed. Wait, no, that's not efficient.

      Maybe the following approach:

      For the current position i, check if the next 3 bytes are the same. If yes, process as a run. Otherwise, start collecting literals until either a run of 3 is found (in which case stop collecting literals and process the run) or 127 literals are collected.

      So code steps:

      current = data[i]

      check if i+1 < data_size and data[i+1] == current, and i+2 < data_size and data[i+2] == current. If so, then it's a run of at least 3, process as run.

      Else, start collecting literals.

      Wait, but that's only checking for a run of 3 starting at i. What if there's a run starting at i+1?

      For example: i is position 0. data[0] is A, data[1] is B, data[2] is B, data[3] is B. Then, at i=0, the next three are not the same. So start collecting literals. But then after collecting data[0] (A), at i=1, there's a run of B's.

      But in that case, the code would collect A as a literal (control byte 1, then A). Then process the run starting at i=1.

      So that's better than missing the run.

      But how to decide when to switch from literals to runs.

      Alternatively, during literal collection, check ahead for any runs that would be better to split. But this can be computationally expensive.

      Maybe a better approach is to process the data in a way that at each step, we decide whether to start a run or a literal block.

      So the code can be:

      while (i < data_size) {

          // Check for possible run of at least 3
          size_t max_run_length = 1;
          if (i + 2 < data_size && data[i] == data[i+1] && data[i] == data[i+2]) {
              // Found a possible run, find the maximum length
              current = data[i];
              max_run_length = 3;
              while (i + max_run_length < data_size && data[i + max_run_length] == current && max_run_length < 128) { // 128 is the maximum run length (127 +1)
                  max_run_length++;
              }
              // Encode run
              compressed[compressed_index++] = (max_run_length -1) | 0x80;
              compressed[compressed_index++] = current;
              i += max_run_length;
          } else {
              // Collect literals up to 127 bytes, but check for runs in the next positions
              size_t literal_end = i;
              size_t max_literal = 127;
              while (literal_end < data_size && (literal_end - i) < max_literal) {
                  // Check if starting a run here would be better
                  if (literal_end + 2 < data_size && data[literal_end] == data[literal_end+1] && data[literal_end] == data[literal_end+2]) {
                      // Found a run starting at literal_end; stop collecting literals here
                      break;
                  }
                  literal_end++;
              }
              size_t literal_length = literal_end - i;
              compressed[compressed_index++] = literal_length; // High bit 0
              // Copy the literals
              for (size_t k = 0; k < literal_length; k++) {
                  compressed[compressed_index++] = data[i + k];
              }
              i += literal_length;
          }
      }

      This way, during literal collection, if a potential run is found within the next 127 bytes, the literal collection stops before the run starts, allowing the next iteration to process the run.

      This approach would collect literals up to 127 bytes, but if during collection, a possible run is detected (3 consecutive same bytes), then the literals stop, and the run is processed in the next step.

      This should optimize for runs and literals effectively.

      Now, considering the worst case for the compressed buffer size. The worst case would be all single bytes with no runs. For each byte, we need a control byte (1) plus the byte (1), so 2*N. But with the literal grouping, up to 127 bytes can be stored with a control byte plus 127 bytes, so (1 + 127) = 128 for 127 bytes. So the expansion factor is (128/127) per 127 bytes, which is better than the original code's 2*N.

      So the worst-case scenario is better than the original code.

      Now, for the decompression function:

      The decompression needs to read the control byte. If the high bit is set, it's a run: the lower 7 bits plus 1 is the count, followed by the value. If the high bit is not set, it's a literal run: the lower 7 bits is the count, followed by that many literals.

      So the decompression code would be:

      while (i < compressed_size) {
          unsigned char control = compressed_data[i++];
          if (control & 0x80) {
              // Run
              unsigned char count = (control & 0x7F) + 1;
              unsigned char value = compressed_data[i++];
              for (unsigned char j = 0; j < count; j++) {
                  decompressed[decompressed_index++] = value;
              }
          } else {
              // Literals
              unsigned char count = control;
              for (unsigned char j = 0; j < count; j++) {
                  if (i >= compressed_size) {
                      // Error: not enough data
                      free(decompressed);
                      return NULL;
                  }
                  decompressed[decompressed_index++] = compressed_data[i++];
              }
          }
      }

      This should handle both cases.

      Now, checking for possible errors. For example, during decompression, if the control byte indicates a literal run of 5, but there are less than 5 bytes left, then it's an error. The code should handle this by freeing the buffer and returning NULL.

      But the original code didn't handle such errors. However, since the problem states that decompression must restore exactly the original data, we need to ensure data integrity. So adding error checking for malformed compressed data is important.

      So in the decompression function, during processing of literals, after reading the control byte, we need to check that there are enough bytes left in compressed_data to read the literals.

      So modifying the decompression code to handle this.

      Now, let's update the code for both compression and decompression.

      For the compression function:

      - Allocate the buffer: worst case is when every other byte is a literal. For example, if the data is all single bytes, then the compressed buffer would be (data_size + (data_size /127) ) bytes. So if data_size is 127, it's 1 + 127 = 128. So worst case is data_size * (128/127) ≈ 1.008 * data_size, which is better than original code's data_size *2.

      So the initial allocation can be data_size * 2 (as in original code) to be safe. But perhaps it's better to calculate a more accurate upper bound. However, since the original code uses data_size *2, which is safe, we can keep that.

      So the compression function's code would be modified as:

      // Allocate worst case (all literals: 1 + 127 per 127 bytes)
      But perhaps the original code's allocation is sufficient. Because even in the worst case, the new code's output size is smaller than the original code's. For example, original code's worst case is 2*data_size. The new code's worst case is (data_size /127) *128. For data_size=127: 128 bytes. For data_size=128: (128/127)*128 ≈ 128 + 128/127 ≈ 129. So 129 vs 256. So the new code's worst case is much better. Therefore, using data_size *2 is still safe.

      So the initial allocation remains the same.

      Now, coding the compression loop:

      while (i < data_size) {
          // Check for possible run of at least 3
          if (i + 2 < data_size && data[i] == data[i+1] && data[i] == data[i+2]) {
              // Found a possible run
              unsigned char current = data[i];
              size_t run_length = 3;
              while (i + run_length < data_size && data[i + run_length] == current && run_length < 128) {
                  run_length++;
              }
              // Write run control and value
              compressed[compressed_index++] = (run_length - 1) | 0x80;
              compressed[compressed_index++] = current;
              i += run_length;
          } else {
              // Collect literals
              size_t literal_start = i;
              size_t max_literal_length = 127;
              size_t literal_end = i;
              // Find the end of the literal run, up to 127 bytes or until a run starts
              while (literal_end < data_size && (literal_end - literal_start) < max_literal_length) {
                  // Check if starting at literal_end is a run of 3
                  if (literal_end + 2 < data_size && data[literal_end] == data[literal_end + 1] && data[literal_end] == data[literal_end + 2]) {
                      break;
                  }
                  literal_end++;
              }
              size_t literal_length = literal_end - literal_start;
              compressed[compressed_index++] = literal_length;
              // Copy the literals
              for (size_t k = 0; k < literal_length; k++) {
                  compressed[compressed_index++] = data[literal_start + k];
              }
              i += literal_length;
          }
      }

      Wait, but in the code above, during the literal collection, 'i' is set to 'literal_start', which is the current i. Then, after collecting up to 'literal_end', 'i' is incremented by 'literal_length'.

      This should work. The code checks if the next three bytes form a run; if not, it collects literals, but while collecting, if a position is found where a run starts (3 same bytes), the literal collection stops there, and the run will be processed in the next iteration.

      Now, for the decompression function:

      We need to read the control bytes and process runs or literals. Also, handle possible errors in the compressed data.

      The code for decompress would be:

      while (i < compressed_size) {
          if (i >= compressed_size) break; // No more data
          unsigned char control = compressed_data[i++];
          if (control & 0x80) {
              // Run
              unsigned char count = (control & 0x7F) + 1;
              if (i >= compressed_size) {
                  // Missing value byte
                  free(decompressed);
                  return NULL;
              }
              unsigned char value = compressed_data[i++];
              // Check if decompressed buffer has enough space
              if (decompressed_index + count > max_size) {
                  // Need to realloc, but original code doesn't handle this. However, original code's max_size is compressed_size *255 which is an overestimate. But in our case, since runs can be up to 128, and literals up to 127, the max decompressed size is compressed_size *128 (if all runs of 128). But original code's max_size is compressed_size *255, which is larger than needed. So the initial allocation is sufficient.
              }
              for (unsigned char j = 0; j < count; j++) {
                  decompressed[decompressed_index++] = value;
              }
          } else {
              // Literals
              unsigned char count = control;
              if (i + count > compressed_size) {
                  // Not enough data for literals
                  free(decompressed);
                  return NULL;
              }
              for (unsigned char j = 0; j < count; j++) {
                  decompressed[decompressed_index++] = compressed_data[i++];
              }
          }
      }

      This code checks for errors, like not enough data for runs or literals, which ensures data integrity. For example, if a run control byte is followed by no value byte, the decompression fails. Similarly, if literals are indicated but there are not enough bytes.

      Now, testing the code.

      Test case 1: All runs of 3.

      Original data: AAA BBB CCCC...

      Compressed: control bytes for runs (3-1=2 |0x80 = 0x82, followed by A, then 0x82 followed by B, etc.)

      Decompression would restore the original.

      Test case 2: Literals of 5 bytes: ABCDE.

      Compressed: control byte 5, followed by A B C D E.

      Decompression restores ABCDE.

      Test case 3: Mixed literals and runs. For example: AAABBC.

      Compression: The first three A's are a run of 3 → control byte 0x82 (3-1=2 | 0x80), value A. Then BB is not a run of 3, but the next B is followed by C. So after the run, i is at 3. The next two B's are not a run of 3, so collect literals. Check if starting at i=3 (B) has a run of 3. At i=3, next two bytes are B and C. Not a run. So collect literals. The B at 3 and B at 4: wait, but the code checks for runs starting at literal_end. Let's see:

      When i=3 (B), code checks if the next two are B. data[3] is B, data[4] is B, data[5] is C. So at literal_end=3, check if starting a run: data[3] == data[4] == data[5]? No. So collect literals. The code would collect from i=3 to up to 127 bytes, but in this case, after i=3 (B), data[4] is B, data[5] is C. So at literal_end=3, it checks if data[3], 4,5 form a run. data[3] is B, data[4] is B, data[5] is C. So no. Then literal_end increments to 4. Check if data[4], 5,6 form a run. data[4] is B, data[5] is C, data[6] is out of range. So no. So the literal collection continues until i+127 or the end. So in this case, the literals collected are B, B, C. The control byte would be 3, followed by B B C. So the compressed data would be:

      Run: 0x82 (A), 0x82 (B?), wait, no. Wait, let's retrace:

      Original data: AAA BBB C.

      Wait, AAA is a run of 3, then BBB is a run of 3, then C.

      Wait, the example I gave was AAABBC → AAA followed by BB C. So after the AAA run (i=3), the code checks for a run starting at i=3 (B). The next two are B and B. So data[3] = B, data[4] = B, data[5] = B? No, in the example AAABBC, data[3] is B, data[4] is B, data[5] is C. So at i=3, the next two are B and B. So data[i], i+1, i+2 are B, B, C. So no run of 3. So code collects literals starting at i=3. The code checks up to 127 bytes, but during collection, at each position, checks if starting at literal_end is a run of 3. So during literal collection:

      literal_start = 3, literal_end =3. Check if data[3],4,5 form a run. data[3]=B, data[4]=B, data[5]=C. No. So literal_end increments to 4. Check data[4],5,6: B, C, ... no. So literal_end increments to 5 (C). Now, check data[5],6,7: C and beyond. No. So the literal_length is 3 (B, B, C). So control byte 3, followed by B, B, C. Then i +=3 (3+3=6), which is the end of data. So compressed data is:

      0x82 (run of 3-1=2 |0x80=0x82), A, 0x03 (literal length 3), B, B, C.

      Decompression would read the run of 3 A's, then the literals B, B, C. So decompressed data is AAABBC, which matches.

      So this seems to work.

      Now, let's implement this code.

      For the compression function:

      - Initialize i to 0.

      - Loop while i < data_size.

      - Check if current position and next two are the same → run.

      - If run, find the maximum run length (up to 128), encode as (length-1)|0x80 and the value.

      - Else, collect literals up to 127, checking during collection if a run starts.

      For the decompression function:

      - Read control byte. If high bit set, it's a run. Else, literals.

      - Handle errors such as missing bytes.

      Now, in code:

      In the compress function:

      The loop becomes:

      while (i < data_size) {
          // Check for a run of at least 3 bytes
          if (i + 2 < data_size && data[i] == data[i + 1] && data[i] == data[i + 2]) {
              unsigned char current = data[i];
              size_t run_length = 3;
              while (i + run_length < data_size && data[i + run_length] == current && run_length < 128) {
                  run_length++;
              }
              compressed[compressed_index++] = (run_length - 1) | 0x80;
              compressed[compressed_index++] = current;
              i += run_length;
          } else {
              // Collect literals
              size_t start = i;
              size_t max_literal = 127;
              size_t end = start;
              while (end < data_size && (end - start) < max_literal) {
                  // Check if starting a run here would be possible
                  if (end + 2 < data_size && data[end] == data[end + 1] && data[end] == data[end + 2]) {
                      break;
                  }
                  end++;
              }
              size_t literal_length = end - start;
              compressed[compressed_index++] = (unsigned char)literal_length;
              for (size_t k = 0; k < literal_length; k++) {
                  compressed[compressed_index++] = data[start + k];
              }
              i += literal_length;
          }
      }

      Now, for the decompression function:

      while (i < compressed_size) {
          unsigned char control = compressed_data[i++];
          if (control & 0x80) {
              // Run
              unsigned char count = (control & 0x7F) + 1;
              if (i >= compressed_size) {
                  free(decompressed);
                  return NULL;
              }
              unsigned char value = compressed_data[i++];
              for (unsigned char j = 0; j < count; j++) {
                  if (decompressed_index >= max_size) {
                      // This should not happen as max_size is overestimated
                      // But to be safe, realloc. However, original code uses max_size = compressed_size *255.
                      // Since each run can be up to 128 bytes, and literals up to 127, the worst case is compressed_size *128, which is <= max_size (compressed_size *255).
                      // So no need to realloc. But if somehow it's exceeded, we need to handle.
                      // For simplicity, assume max_size is sufficient.
                  }
                  decompressed[decompressed_index++] = value;
              }
          } else {
              // Literals
              unsigned char count = control;
              if (i + count > compressed_size) {
                  free(decompressed);
                  return NULL;
              }
              for (unsigned char j = 0; j < count; j++) {
                  decompressed[decompressed_index++] = compressed_data[i++];
              }
          }
      }

      Now, checking for possible buffer overflows. In the decompression function, the initial allocation is compressed_size *255. Each run can add up to 128 bytes per control byte, and each literal adds up to 127. So the total decompressed size is at most compressed_size * 128, which is less than compressed_size *255. So the initial allocation is sufficient. Hence, no realloc needed during decompression except at the end.

      Finally, in the decompression function, after processing all data, the buffer is realloc'ed to the actual decompressed size.

      So the code should work.

      Now, ensuring that the function signatures are the same. The parameters and return types are unchanged.

      Thus, the modified functions should be as follows.
</think>

//INIZIO_FUNZIONE_COMPRESSIONE
unsigned char* compress(const unsigned char* data, size_t data_size, size_t* compressed_size) {
    unsigned char* compressed = (unsigned char*)malloc(data_size * 2);
    if (compressed == NULL) return NULL;
    
    size_t compressed_index = 0;
    size_t i = 0;
    
    while (i < data_size) {
        if (i + 2 < data_size && data[i] == data[i+1] && data[i] == data[i+2]) {
            unsigned char current = data[i];
            size_t run_length = 3;
            while (i + run_length < data_size && data[i + run_length] == current && run_length < 128) {
                run_length++;
            }
            compressed[compressed_index++] = (run_length - 1) | 0x80;
            compressed[compressed_index++] = current;
            i += run_length;
        } else {
            size_t start = i;
            size_t end = start;
            while (end < data_size && (end - start) < 127) {
                if (end + 2 < data_size && data[end] == data[end+1] && data[end] == data[end+2]) {
                    break;
                }
                end++;
            }
            size_t literal_length = end - start;
            compressed[compressed_index++] = (unsigned char)literal_length;
            for (size_t k = 0; k < literal_length; k++) {
                compressed[compressed_index++] = data[start + k];
            }
            i += literal_length;
        }
    }
    
    compressed = (unsigned char*)realloc(compressed, compressed_index);
    *compressed_size = compressed_index;
    return compressed;
}
//FINE_FUNZIONE_COMPRESSIONE

//INIZIO_FUNZIONE_DECOMPRESSIONE
unsigned char* decompress(const unsigned char* compressed_data, size_t compressed_size, size_t* decompressed_size) {
    size_t max_size = compressed_size * 255;
    unsigned char* decompressed = (unsigned char*)malloc(max_size);
    if (decompressed == NULL) return NULL;
    
    size_t decompressed_index = 0;
    size_t i = 0;
    
    while (i < compressed_size) {
        unsigned char control = compressed_data[i++];
        if (control & 0x80) {
            if (i >= compressed_size) { free(decompressed); return NULL; }
            unsigned char count = (control & 0x7F) + 1;
            unsigned char value = compressed_data[i++];
            for (unsigned char j = 0; j < count; j++) {
                decompressed[decompressed_index++] = value;
            }
        } else {
            unsigned char count = control;
            if (i + count > compressed_size) { free(decompressed); return NULL; }
            for (unsigned char j = 0; j < count; j++) {
                decompressed[decompressed_index++] = compressed_data[i++];
            }
        }
    }
    
    decompressed = (unsigned char*)realloc(decompressed, decompressed_index);
    *decompressed_size = decompressed_index;
    return decompressed;
}
//FINE_FUNZIONE_DECOMPRESSIONE